---
title: "NEOs Classification"
author: "Alberto Sinigaglia, Beatrice Sofia Bertipaglia, Chiara Colato, Flavia Gianfrate"
date: "20/7/2022"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---



## Project Presentation

In this project we face a *binary classification* problem of celestial bodies called *NEOs*. These are objects belonging to the Solar System whose orbit can intersect the Earth's orbit and therefore can represent a danger for our planet. NEOs are therefore classified into "Hazardous" and "Not Hazardous", based on parameters that take into account their potential approach to the Earth, represented by the attributes of the proposed dataset. The aim of this analysis is therefore to define whether these celestial bodies can be dangerous or not for the Earth and to understand which 
characteristics are useful for this goal.

## Dataset Presentation and Preprocessing

The proposed dataset consists of 4687 objects whose characteristics are described by 40 attributes. These are orbital parameters with mostly real values, suitable for describing the orbit of the body and its structure. The variable that describes the danger of the object is presented, on the other hand, as a logical variable and it's called *Hazardous*. 
We proceed with the importation of the data and necessary libraries.

```{r, include = TRUE, message = FALSE}
# Retrieval of the libraries necessary for the correct import of 
# the data, manipulation and data visualization

library(tidyverse)
library(ggplot2)
library(gridExtra)
library(tidymodels)
library(leaps)
library(glmnet)
library(pROC)
library(rsample)
library(correlation)
library(DataExplorer)
library(knitr)
library(corrplot)
library(regclass)
library(rsample)
library(corrplot)
library(outliers)
library(dplyr)
library(caret)
library(class)

```

### Data Uploading

```{r, messagge = FALSE}

# We upload the dataset "nasa.csv" with the function "read_delim" of 
# the tidyverse package:

nasa_orig <- read_delim("nasa.csv", delim = ",")

# We check if the upload is correct looking at the first 5 rows

head(nasa_orig,5)

# We rename the columns' names in order to able to recall them: the
# presence of spaces, parenthesis or points could be a problem, so we
# avoid the issues introducing instead the character "_"

names(nasa_orig)<- gsub("\\s","_",names(nasa_orig))

```

### Data Pre-Processing

Before proceeding with the analysis we explore the structure of the dataset we want to study.

The first operation we do is to check the presence of missing values in the dataset: if the output returned is *FALSE* we don't have any missing value, so we can proceed.

```{r, include = TRUE}
anyNA(nasa_orig) 

```

We consider now the features described in the dataset that represent the characteristics for each unit. 
We searched for the meaning of each one and we report the descriptions in the following table:

```{r, include = FALSE}
list_variables <- c("Neo Reference ID","Name","Absolute Magnitude"," ","Est Dia in KM(min)","Est Dia in KM(max)","Est Dia in M(min)","Est Dia in M(max)","Est Dia in Miles(min)","Est Dia in Miles(max)","Est Dia in Feet(min)","Est Dia in Feet(max)",         "Close Approach Date","Epoch Date Close Approach","Relative Velocity km per sec","Relative Velocity km per hr","Miles per hour","Miss Dist.(Astronomical)","Miss Dist.(lunar)","Miss Dist.(kilometers)", "Miss Dist.(miles)","Orbiting Body","Orbit ID","Orbit Determination Date","Orbit Uncertainity"," "," ","Minimum Orbit Intersection","Jupiter Tisserand Invariant"," "," ","Epoch Osculation","Eccentricity"," ","Semi Major Axis","Inclination"," "," ","Asc Node Longitude"," ","Orbital Period"," ","Perihelion Distance","Perihelion Arg"," ","Aphelion Dist","Perihelion Time","Mean Anomaly"," ","Mean Motion"," ","Equinox","Hazardous")
  
list_descriptions<- c("ID of the object",
                      "Object name",
                      "Absolute magnitude is a measure of the intrinsic brightness of an object that",
                      "does not consider its brightness variations due to real conditions",
                      "Minimum estimated diameter in km",
                      "Maximum estimated diameter in km",
                      "Minimum estimated diameter in m",
                      "Maximum estimated diameter in m",
                      "Minimum estimated diameter in Miles",
                      "Maximum estimated diameter in Miles",
                      "Minimum estimated diameter in Feet",
                      "Maximum estimated diameter in Feet",
                      "Date where the object is nearer to Earth", 
                      "Coordinates where the object is nearer to Earth",
                      "Velocity in km/s",
                      "Velocity in km/hr",
                      "Velocity in Miles/hr",
                      "Distance from Earth where the object passes in AU",
                      "Distance from Earth where the object passes in LU",
                      "Distance from Earth where the object passes in km",
                      "Distance from Earth where the object passes in Miles",
                      "Body around which the object orbits",
                      "Orbit ID of the object",
                      "Date when the orbit was defined",
                      "Uncertainity measure of the orbit, related to several parameters used in the orbit", 
                      "determination process (as the number of measurements, the time spanned by those",
                      "observations, their quality and the geometry of the observations",
                      "Minimum distance between object's orbit and  Earth orbit",
                      "Tisserand parameter (or Tisserand invariant), is a value calculated from several",
                      "orbital elements (semi-major axis, eccentricity and orbital inclination) of a",
                      "relatively small object. It is used to distinguish different types of orbits",
                      "Time to which the data refer",
                      "The eccentricity can be considered as the measure of how far the orbit is deviated",
                      "from a circle",
                      "Semi-major axis length in AU",
                      "Inclination is one of the orbital parameters that describe the shape and orientation",
                      "of an orbit: it's the angular distance of the orbital plane from the reference plane",
                      "expressed in degrees",
                      "The ascending node is the point where the orbit of the object passes through the",
                      "plane of reference",
                      "The orbital period is the amount of time a given astronomical object takes to",
                      "complete one orbit around another object",
                      "Maximum distance of the object from the Earth",
                      "Parametrically, it is the angle from the ascending node of the body measured in",
                      "the direction of movement",
                      "Minimum distance of the object from the Earth",
                      "Moment when the object is in perihelion",
                      "In celestial mechanics, the mean anomaly is the fraction of an elliptical orbit",
                      "period that has elapsed since the orbiting body passed periapsis",
                      "Mean motion is used as an approximation of the actual orbital speed in making an",
                      "initial calculation of the body's position in its orbit",
                      "Time to which the data refer",
                      "Dangerousness of the object - response variable")

variables_df<- data.frame(list_variables, list_descriptions)

colnames(variables_df)<- c("Variable", "Description")
```

```{r, echo = FALSE}
kable(variables_df, "simple")
```


Starting from the set of the original features, some considerations about them need to be done:

* Some variables are redundant, as they are presented for different units of measure;
* Some features are simple identifiers;
* Some features show a unique value for all units and therefore do not bring useful information;

In order to avoid to consider the same information several times, we proceed removing the repeated features, maintaining only one unit measure: for example we choose to consider the measurement of the *Estimated Diameter* in Kilometers (instead of Miles or Feet). Then, we delete also the identifiers *Name* and *ID* because they are not useful for our analysis.

```{r, results = "hide"}
toremove<- c("Neo_Reference_ID","Name", "Est_Dia_in_M(min)", 
             "Est_Dia_in_M(max)", "Est_Dia_in_Miles(min)",
             "Est_Dia_in_Miles(max)", "Est_Dia_in_Feet(min)",
             "Est_Dia_in_Feet(max)", "Close_Approach_Date",
             "Epoch_Date_Close_Approach", 
             "Relative_Velocity_km_per_sec",
             "Miles_per_hour", "Miss_Dist.(lunar)",
             "Miss_Dist.(kilometers)",
             "Miss_Dist.(miles)", "Orbiting_Body", "Orbit_ID", 
             "Orbit_Determination_Date", "Epoch_Osculation",
             "Equinox", "Est_Dia_in_KM(min)")  

nasa<- nasa_orig %>%
       select(-all_of(toremove))

colnames(nasa)[2]<- "Est_Dia_in_KM_max"
colnames(nasa)[4]<- "Miss_Dist_Astronomical"

```

Another thing that we have to check is the type of our features. We look at their nature and what we would like is to codify them in a correct and meaningful way in such a way that we can use them inside models.

```{r, include = TRUE}
# Conversion in correct type:

summary(nasa)

nasa$Orbit_Uncertainity<- nasa$Orbit_Uncertainity %>%
                          as.factor() %>%
                          as.numeric()

nasa$Perihelion_Distance<- nasa$Perihelion_Distance %>%
                           as.factor() %>%
                           as.numeric()

nasa <- nasa %>% mutate(Hazardous = ifelse(Hazardous == TRUE, 1, 0))

nasa$Hazardous<- nasa$Hazardous %>%
                           as.factor() 

# Our response variable is codified now as a factor:
# Hazardous = 0 means that the unit is not considered dangerous for Earth
# Hazardous = 1 means that the unit is considered dangerous for Earth

```

### Data Balance Check

From the summary another issue emerges: the classes *Hazardous* and *Not Hazardous* are not balanced and this could become a problem during the training of the classification models.

```{r, include = TRUE}
prop.table(table(nasa$Hazardous))
```

In order to limit the problem we decide to introduce techniques able to balance the two classes, sampling more from the minority class and less from the majority one. 
In our analysis, after the subdivision of the dataset, we will consider the possibility to balance the training dataset: for the first models we will compare their performances on the original dataset and on the balanced one, in order to understand if the balancing can bring to an improvements of the performances.

## Splitting in Training e Test Sets

Before continuing to explore the properties of our data, we divide them into two different datasets: the *training dataset* and the *test dataset* that will be used respectively for training our models and for testing the models' performances.
We proceed with all the considerations about the features focusing on the training one, trying to understand the relations between them. The test set will remain "unknown". For doing the split, the proportions defined are 0.75 and 0.25, respectively for the two sets.

```{r, include = TRUE}
# Train-Test Split 

# We set a fixed seed in order to be able to reproduce the same
# results each time we run the code:
set.seed(0607)

split <- initial_split(nasa, prop = 0.75) 
train <- training(split)
test <- testing(split)

# We check if the proportion of the Hazardous NEOs is the same in all the datasets: 
# we want to have datasets that represent the original situation.

# Proportion of Hazardous and Not Hazardous in the training dataset:

prop.table(table(train$Hazardous))

# Proportion of Hazardous and Not Hazardous in the test dataset:

prop.table(table(test$Hazardous))
```

## Data Analysis

We focus now on the training dataset in order to explore the relations between the different features and the relation between the features and the response variable. This procedure can be useful for evaluating from an objective point of view the connections between the available characteristics we have and for finding a solution to our problem.

### Correlations
The first step of our data analysis consists into the analysis of *Correlation*, statistical measure used to quantify the strength of the linear relationship between two variables and compute their association. We are basically trying to observe the level of change in one variable due to the change in the other one. Although we are computing only the linear connection between the covariates, this can be a useful starting point.
First, we can look at the correlations between our response variable *Hazardous* and the covariates. At the beginning we consider *Hazardous* as a numeric variable in order to have a path to follow in this correlation analysis, but ,because in our problem *Hazardous* is a factor, then we proceed plotting the density of the most correlated features, separating the case for bodies classified as *Hazardous* and *Not Hazardous*.

```{r, include = FALSE}
list_var <- c("Absolute Magnitude", "Orbit_Uncertainity", "Minimum Orbit Intersection","Perihelion Distance ","Eccentricity","Relative Velocity" )

list_corr<- c(-0.32, -0.32, -0.29, -0.21, 0.19, 0.18)

corr_df<- data.frame(list_var, list_corr)

colnames(corr_df)<- c("Variable", "Correlation with Hazardous")
```

```{r, echo = FALSE}
kable(corr_df, "simple")
```

```{r, fig.show = "hide"}

Haz<- train$Hazardous

# Absolute Magnitude 
a<- ggplot(train, aes(x = Absolute_Magnitude, fill = Haz)) + 
  geom_density(alpha = 0.4)+
  ggtitle("Absolute Magnitude - Density Plot") +
  xlab("Absolute Magnitude")

# Orbit Uncertainity 

b<- ggplot(train, aes(x = Orbit_Uncertainity, fill = Haz)) +
  geom_bar(aes(y = ..prop..),position = "dodge") +
  ggtitle("Orbit Uncertainity - Bar Plot") +
  xlab("Orbit Uncertainity")

# Minimum Orbit Intersection
c<- ggplot(train, aes(x = Minimum_Orbit_Intersection, fill = Haz)) +
  geom_density(alpha = 0.4) +
  xlim(0, 0.5) +
  ggtitle("Minimum Orbit Intersection - Density Plot") +
  xlab("Minimum Orbit Intersection")

# Eccentricity
d<- ggplot(train, aes(x = Eccentricity, fill = Haz)) +
  geom_density(alpha = 0.4)+
  ggtitle("Eccentricity - Density Plot") +
  xlab("Eccentricity")

# Perihelion Distance
e<- ggplot(train, aes(x = Perihelion_Distance, fill = Haz)) +
  geom_density(alpha = 0.4) +
  ggtitle("Perihelion Distance - Density Plot") +
  xlab("Perihelion Distance")

# Relative Velocity
f<- ggplot(train, aes(x = Relative_Velocity_km_per_hr, fill = Haz)) +
  geom_density(alpha = 0.4) +
  ggtitle("Relative Velocity - Density Plot") +
  xlab("Relative Velocity")


grid.arrange(a, b, c, d, e, f, nrow = 2)

```

![Density plots for NEOs hazardous and not hazardous](./plot_var_haz.png)

As we note, we see that the values of *Absolute Magnitude* distribute differently for NEOs dangerous and not dangerous for Earth; the same seems to hold for *Minimum Orbit Intersection* and *Orbit Uncertainity*.
Looking at the *Eccentricity* plot, we see that the mean of the distribution changes as the level of *Hazardous* changes, and, considering that we are looking at the range (0,1), translations of the mean from 0.25 to 0.5 can be considered relevant differences. Proceeding with the plots we can observe also that, generally, for bodies which are
classified as potentially dangerous we have less values of *Perihelion Distance*. The *Relative Velocity*, instead, doesn't seem to differentiate well the two classes of objects.

These initial information leads to the identification of a dangerous NEO as a body with a medium-low absolute magnitude, with a degree of Uncertainity of the orbit and minimum distance of intersection with the Earth orbit generally low. We can also say that, usually, these are bodies that show a relative speed a little greater than a body not considered dangerous and that generally show a shorter perihelion distance.

Focusing now on the correlations between the covariates, we can build the following matrix:
```{r, include = TRUE, fig.cap = "Correlation matrix between the covariates"}
# We use the "corrplot" function that represents the correlation in a graphic way:

corrplot(cor(train[,-19]),
         method = "number",
         diag = FALSE,
         tl.cex = 0.4,
         number.cex = 0.5,
         tl.col = "black")
```

We note that we have some variables that are highly correlated between each other. This can represent a case of *Collinearity* between the attributes: we try to understand the nature of these relationships  studying deeper the meaning of the variables mostly involved.


### Partial Correlations

Thanks to the *Partial Correlation* we can investigate the relationship between two variables without the influence of any other relationship between them and other attributes present in the dataset.

```{r, results = "hide"}
# We use the "correlation" function from the "correlation" package, where we can specify 
# the computation of the partial correlations

correlation(train, partial = TRUE)

# (The output is not shown for its dimensions, but we resume the highest
# values into the following table).
```

We report in the following table the highest individuated correlations:

```{r, include = FALSE}
list_var1<- c("Mean Motion", "Mean Motion", "Mean Motion", "Mean Motion","Aphelion Distance", "Aphelion Distance", "Aphelion Distance", "Aphelion Distance", "Aphelion Distance", "Perihelion Distance", "Perihelion Distance", "Perihelion Distance", "Orbital Period", "Orbital Period", "Inclination", "Semi Major Axis", "Semi Major Axis", "Eccentricity", "Orbit Uncertainity", "Estimated Diameter KM max")

list_var2<- c("Jupiter Tisserand Parameter", "Semi Major Axis", "Orbital Period", "Aphelion Distance", "Jupiter Tisserand Parameter", "Mean Motion", "Eccentricity", "Semi Major Axis", "Orbital Period", "Jupiter Tisserand Parameter", "Mean Motion", "Semi Major Axis", "Jupiter Tisserand Parameter", "Semi Major Axis", "Relative Velocity", "Jupiter Tisserand Parameter", "Eccentricity", "Relative Velocity", "Absolute Magnitude", "Absolute Magnitude")

list_parcor<- c(0.99, -0.9, -0.86, -0.84, -0.89, 0.84, 0.7, 0.97, 0.98, -0.55, -0.6, 0.51, -0.9, 1, 0.51, -0.93, 0.53, 0.5, 0.67, -0.6)

parcor_df<- data.frame(cbind(list_var1, list_var2, list_parcor))
colnames(parcor_df)<- c("Variable 1", "Variable 2", "Partial Correlation")
```

```{r, echo = FALSE}
kable(parcor_df, "simple")
```

In order to understand  why such variables are so correlated, we investigate more on their formulations.

*   First we analyze the relations where is involved the value of *Jupiter Tisserand Parameter*. In particular we have that the formula for computing it contains the information about the *Semi Major Axis* *a*, the *Eccentricity* *e* and the *Inclination* *i* of the orbit:

![](./formula1.png){#id .class width=220 height=70}

Knowing it, we can explain clearly why there's the presence of a so high correlation between such an invariant and the mentioned attributes.

* Then we analyze the relation between *Mean Motion* and the *Orbital Period*: the mean motion is simply computed dividing one revolution for the orbital period. This explains their negative correlation.

* We consider now the *Orbital Period* quantity. In its formulation is considered the length of the *Semi Major Axis* and for that reason we have a so high correlation between them.

* We note also the relation between *Absolute Magnitude* and the *Estimated Diameter*. In order to better define their relationship we can observe the formula with which they are connected:

![](./formula2.png){#id .class width=150 height=70}

where *p* corresponds to the *albedo* and *H* represents the *Absolute Magnitude*. We can see, in fact, that they are inversely proportional, and so we can explain the negative correlation.

* Then we have also that the value of the *Orbit Uncertainity* is positively correlated with the *Absolute Magnitude* of a body. We know that the smaller the magnitude value, the brighter the body, so can be reasonable to think that to grater values of magnitude correspond darker bodies for which is difficult to define perfectly their orbit trajectory.


These relations can be clearly represented by the following plots:

```{r, include = TRUE, fig.show = "hide"}
a1<- ggplot(data = train, aes(Jupiter_Tisserand_Invariant, Semi_Major_Axis)) + 
  geom_jitter(color = "dark green") +
  xlab("Jupiter Tisserand Invariant") +
  ylab("Semi-Major Axis")

a2<- ggplot(data = train, aes(Mean_Motion, Orbital_Period)) +
  geom_jitter(color = "dark green") +
  xlab("Mean Motion") +
  ylab("Orbital Period")

a3<- ggplot(data = train, aes(Orbital_Period, Semi_Major_Axis)) +
  geom_jitter(color = "dark green")  +
  xlab("Orbital Period") + 
  ylab("Semi-Major Axis")

a4<- ggplot(data = train, aes(Absolute_Magnitude, Est_Dia_in_KM_max))+
  geom_jitter(color = "dark green") +
  xlab("Absolute Magnitude") +
  ylab("Estimated Diameter")

a5<- ggplot(data = train, aes(as.factor(Orbit_Uncertainity), Absolute_Magnitude, 
                              fill = as.factor(Orbit_Uncertainity))) +
  geom_boxplot() +
  labs(x = "Orbit Uncertainity", y = " Absolute Magnitude") + 
  theme(legend.position = "none") 
  

grid.arrange(a1, a2, a3, a4, nrow = 2)
a5
```
![](./greenplots.png)
![](./OU_AM_boxplot.png){#id .class width=80% height=80%}

All this existent connections, as we resume in the Figure 3, between the covariates could become a problem during the definition of the classification models. For each case, in fact, we will discuss how to face the presence of such relationships and how to treat them.

![Relations Map for Features](./MAP_RELATION.png)

## Logistic Regression Models Definition

We start our analysis considering the *Logistic Regression Model* for classifying the NEOs into *Hazardous* and *Not Hazardous* classes. In particular we try to build our model on the original training dataset and on the training dataset that we balance, in order to have at most the same quantities of dangerous and not dangerous example. We also
consider the possibility to apply a *Stepwise Selection* approach for including and excluding iteratively the covariates inside the model.
Then, for each built structure we will consider different thresholds for the classification: we are in fact interested into minimize the quantity of *False Negatives*. In order to create a good model useful for the identification of the potentially hazardous bodies, we prefer, in fact,  to have a less values for *Hazardous* NEOs classified as *Not Hazardous*,maintaining at the same time not too high the quantity of the *False Positives*.

```{r, include = TRUE, message = FALSE}

library(ROSE)
library(caret)
library(cowplot)
library(leaps)

# We use the function "ovun.sample" from the "ROSE" package that
# creates possibly balanced samples by random over-sampling
# minority examples and under-sampling majority examples.

train_balanced<- ovun.sample(Hazardous~., data = train, 
                             method = "both", p = 0.5, 
                             N = 3515, seed = 1)$data


# Calling the "data" part of the output we obtain the resulting
# new dataset.


# We define three different thresholds for the classification task:
# 0.4, 0.5, 0.6

threshold4<- 0.4
threshold5<- 0.5
threshold6<- 0.6
```

### Simple Logistic Model

With logistic regression we can estimate the probability of an event happening based of the values of the other variables: here we are estimating the probability of a body to be *Hazardous*, given the values of its attributes.

After an initial definition of the model, we check the presence of collinearity using the *VIF* function that calculates the *Variance Inflation Factor* of all predictors. 
The VIF of a predictor is a measure for how easily it is predicted from a linear regression using the other predictors. In general, a VIF larger than 1/(1-R2), where R2 is the Multiple R-squared of the regression, indicates that predictor is more related to the other predictors than it is to the response.

Considering the computed quantity and removing a feature at time, we reach at the end a situation where we have two variables with high VIF value : *Absolute Magnitude* and *Est_Dia_in_KM_max*. We should delete *Absolute Magnitude* because of its highest value, but we observe that, if we decide for its removal the total errors produced in the classification becomes twice as much the total error of the model that includes the same attribute. 
For this reason we consider the possibility of eliminating *Est_Dia_in_KM_max* instead of *Absolute Magnitude* and see what happens.
In this way, we kept *Absolute Magnitude* in our model which is now less than 1/(1-R2) and we achieved good results in terms of total error of the model, eliminating the collinearity between the variables of our dataset.
Hence all the features considered in the model carry information regarding their relationship to the answer, without considering spurious correlations.

```{r, include = TRUE, warning = FALSE}

# Model definition:

glm_compl<- glm(data = train,
            Hazardous ~ .,
            family = "binomial")

# We compute the reference level R-Squared

s<- summary(glm_compl)
r2<- 1 - (s$deviance/s$null.deviance)

1/(1-r2)

# Using the VIF function and comparing the obtained values with the 
# computed quantity:
# (The process is done iteratively where we delete one variable at time)

VIF(glm_compl)

glm_compl<- glm(data = train,
            Hazardous ~.-Aphelion_Dist-Semi_Major_Axis-
              Jupiter_Tisserand_Invariant-
              Eccentricity-Mean_Motion-Est_Dia_in_KM_max,
            family = "binomial")

# Observation of the model summary:

summary(glm_compl)

# Computing the predictions with the model on the test set: 

pred_glm_compl<- predict(glm_compl, test, type = "response")

# Converting the prediction in {0,1} according to the chosen threshold:

pred_glm_compl_04<- ifelse(pred_glm_compl > threshold4, 1, 0)
pred_glm_compl_05<- ifelse(pred_glm_compl > threshold5, 1, 0)
pred_glm_compl_06<- ifelse(pred_glm_compl > threshold6, 1, 0)
```


We can observe the resulting summary of the model. First we have the
explicitation of the formula where it's possible to see what variables we are including or excluding in the model. Then we move our attention on the estimated coefficient for each of them: the significant attributes seem to be:

* Absolute Magnitude
* Orbit Uncertainity
* Minimum Orbit Intersection
* Inclination
* Orbital Period


In order to know how good the logistic model is classifying we can take
a look to the *Confusion Matrix* built for each threshold:

```{r, include = TRUE, results = "hide", fig.show = "hide"}
# Confusion matrix with threshold = 0.4

table(test$Hazardous, pred_glm_compl_04)
mean(pred_glm_compl_04!=test$Hazardous)

# Confusion matrix with threshold = 0.5

table(test$Hazardous, pred_glm_compl_05)
mean(pred_glm_compl_05!=test$Hazardous)

# Confusion matrix with threshold = 0.6

table(test$Hazardous, pred_glm_compl_06)
mean(pred_glm_compl_06!=test$Hazardous)
```

![](./confusion_glm.png){#id .class width=100% height=100%}


Focusing now on the confusion matrices, we note that increasing the threshold for which we classify a body as *Hazardous*, we risk to produce more *False Negatives*. Instead decreasing the same value we could be able to obtain a model that can individuate the potentially dangers and so it could be more useful for avoiding them.

### Logistic Model with Stepwise Selection

We introduce here, to the previous defined model, the *Stepwise Selection* that consists into a method that iteratively adds and/or removes predictors. The aim is to find the subset of variables in the dataset that results in the best performing model, that is a model that minimizes the AIC quantity.
The *Akaike Information Criterion* (AIC) is an estimator of prediction error and of the relative quality of statistical models for a given set of data. In particular, the main characteristic of this method is the introduction of a *penalization term*, with which the model is penalized as much variables includes. This term, fore some models, including the logistic regression, is computed in a closed form.

The Stepwise can be implemented following three strategies:

* *Forward selection*: it starts with no predictors in the model and it adds the most contributive predictors in an iterative way, stopping when the improvement is no significant.
* *Backward selection*: it starts with all predictors in the model (full model) and iteratively removes the least contributive predictors, stopping when you we a model where all predictors are statistically significant.
* We can also combine both the techniques: we start with no predictors, then sequentially add the most contributive predictors (like forward selection). After adding each new variable, remove any variables that no longer provide an improvement in the model fit (like backward selection).

In our project we use the third option:

```{r, include = TRUE, warning = FALSE}

library(leaps)
library(MASS)

# Model definition:

# Here we don't re-apply the VIF method because we start from the
# previous result.

glm_compl<- glm(data = train,
            Hazardous ~.-Aphelion_Dist-Semi_Major_Axis-
              Jupiter_Tisserand_Invariant-
              Eccentricity-Mean_Motion-Est_Dia_in_KM_max,
            family = "binomial")

# Application of the Stepwise method, specifying that we consider
# both the forward and the backward directions. We consider as 
# reference metric the Akaike Information Criterion: 

glm_compl_step <- stepAIC(glm_compl, direction = "both", 
                          trace = FALSE)

# Observation of the model summary:

summary(glm_compl_step)


# Computing the predictions with the model on the test set:

pred_glm_compl_step = predict(glm_compl_step, test, type = "response")


# Converting the predictions in {0,1} according to the chosen threshold:

pred_glm_compl_step_04 = ifelse(pred_glm_compl_step > threshold4, 1, 0)
pred_glm_compl_step_05 = ifelse(pred_glm_compl_step > threshold5, 1, 0)
pred_glm_compl_step_06 = ifelse(pred_glm_compl_step > threshold6, 1, 0)
```

Also here we take a look to the summary of the model. Here the significant variables are:

* Absolute Magnitude
* Orbit Uncertainity
* Minimum Orbit Intersection
* Inclination
* Orbital Period

As we can note, comparing this result with the previous one, the model considers as significant the same features as before. The only thing that changes is the significance of the intercept. 

```{r, include = TRUE, results = "hide", fig.show = "hide"}

# Confusion matrix with threshold = 0.4

table(test$Hazardous, pred_glm_compl_step_04)
mean(pred_glm_compl_step_04!=test$Hazardous)

# Confusion matrix with threshold = 0.5

table(test$Hazardous, pred_glm_compl_step_05)
mean(pred_glm_compl_step_05!=test$Hazardous)

# Confusion matrix with threshold = 0.6

table(test$Hazardous, pred_glm_compl_step_06)
mean(pred_glm_compl_step_06!=test$Hazardous)

```

![](./confusion_glm_step.png){#id .class width=100% height=100%}

We compare now the *Confusion Matrices* obtained with the simple complete model and with the model that uses the stepwise. What we observe is that basically the results are the same: the difference are very minimum.
We can deduce that the use of the *Stepwise* doesn't bring to better results.


### Logistic Model with Balancing

#### Over-sampling and Under-sampling

In the presence of an unbalanced distribution of the response variable, the learning process can be distorted: the model tends, in fact, to focus on the majority class and ignore rare events. The solution adopted in this project is based on *over-sampling* and *under-sampling*: it consists on a pre-treatment of the data, which has the advantage of
being independent of any classification model and adaptable to many different contexts. The goal is to modify the distribution of the classes so as to alleviate the degree of imbalance. This process could not be useful for every proposed model, so what we will do is to consider for each model its performance deriving from the training on
the original set of data and from the balanced one.

```{r, include = TRUE, messagge = FALSE, warning = FALSE}

# Model definition:

glm_bal<- glm(data = train_balanced,
              Hazardous ~ .,
              family = "binomial")

# We compute the reference level R-Squared

s<- summary(glm_bal)
r2<- 1 - (s$deviance/s$null.deviance)

1/(1-r2)

# Using the VIF function and comparing the obtained values with the 
# computed quantity:
# (The process is done iteratively where we delete one variable at time)

VIF(glm_bal)


glm_bal<- glm(data = train_balanced,
              Hazardous ~.-Aphelion_Dist-Semi_Major_Axis-
              Jupiter_Tisserand_Invariant-Eccentricity-
                Est_Dia_in_KM_max-Mean_Motion,
              family = "binomial")

# Observation of the model summary:

summary(glm_bal)


# Computing the predictions with the model on the test set:

pred_glm_bal<- predict(glm_bal, test, type = "response")


# Converting the predictions in {0,1} according to the chosen threshold:

pred_glm_bal_04<- ifelse(pred_glm_bal > threshold4, 1, 0)
pred_glm_bal_05<- ifelse(pred_glm_bal > threshold5, 1, 0)
pred_glm_bal_06<- ifelse(pred_glm_bal > threshold6, 1, 0)
```

Comparing the resulting significant variables between the *Simple GLM* and the same model applied on the balanced training dataset, we can see immediately note that we have the presence, again of the same significant variables:

* Absolute Magnitude
* Minimum Orbit Intersection
* Orbit Uncertainity
* Orbital Period

```{r, include = TRUE, results = "hide", fig.show = "hide"}

# Confusion matrix with threshold = 0.4

table(test$Hazardous, pred_glm_bal_04)
mean(pred_glm_bal_04!=test$Hazardous)

# Confusion matrix with threshold = 0.5

table(test$Hazardous, pred_glm_bal_05)
mean(pred_glm_bal_05!=test$Hazardous)

# Confusion matrix with threshold = 0.6

table(test$Hazardous, pred_glm_bal_06)
mean(pred_glm_bal_06!=test$Hazardous)
```


![](./confusion_glm_bal.png){#id .class width=100% height=100%}

We can suddenly notice that the total error is increased. In particular we were able to reduce the *False Negative* rate, as we wanted, but, in the while we increased the *False Positive* rate. Although it misclassifies a greater number of observations, this model could be considered "safer" and so preferable for our aim of finding dangerous
bodies for the Earth.


#### Balancing with weights

We can propose another method for balancing the classes: it consists into add different weights to elements belonging to different classes and in particular we want to valorize examples of the minority one. For that reason, we apply a weight greater than 1 to the *Hazardous* class.

```{r, include = TRUE, messagge = FALSE, warning = FALSE}

# Definition of the weights

w<- rep(1, nrow(train))

sum(train$Hazardous==0)/sum(train$Hazardous==1)

w[train$Hazardous == 1]<- 5

# Model definition:

glm_weighted<- glm(data = train,
              Hazardous ~ .,
              family = "binomial", weights = w)

# We compute the reference level R-Squared

s<- summary(glm_weighted)
r2<- 1 - (s$deviance/s$null.deviance)

1/(1-r2)

# Using the VIF function and comparing the obtained values with the 
# computed quantity:
# (The process is done iteratively where we delete one variable at time)

VIF(glm_weighted)


glm_weighted<- glm(data = train_balanced,
              Hazardous ~.-Aphelion_Dist-Semi_Major_Axis-
              Jupiter_Tisserand_Invariant-Eccentricity-
              Mean_Motion-Est_Dia_in_KM_max,
              family = "binomial", weights = w)

# Observation of the model summary:

summary(glm_weighted)

# Computing the predictions with the model on the test set:

pred_glm_weighted<- predict(glm_weighted, test, type = "response")

# Converting the predictions in {0,1} according to the chosen threshold:

pred_glm_weighted_04<- ifelse(pred_glm_weighted > threshold4, 1, 0)
pred_glm_weighted_05<- ifelse(pred_glm_weighted > threshold5, 1, 0)
pred_glm_weighted_06<- ifelse(pred_glm_weighted > threshold6, 1, 0)
```
As we can see from the summary above, we note that here we have the introduction of two different variables, that are: *Missing Distance* and *Inclination*, there isn't instead the measure of the *Orbital Period*

* Absolute Magnitude
* Missing Distance
* Orbit Uncertainity
* Minimum Orbit Intersection
* Inclination


We look now at the confusion matrices in order to understand if this new model could be a good alternative to the two previous proposed:

```{r, include = TRUE, results = "hide", fig.show = "hide"}
# Confusion matrix with threshold = 0.4

table(test$Hazardous, pred_glm_weighted_04)
mean(pred_glm_weighted_04!=test$Hazardous)

# Confusion matrix with threshold = 0.5

table(test$Hazardous, pred_glm_weighted_05)
mean(pred_glm_weighted_05!=test$Hazardous)

# Confusion matrix with threshold = 0.6

table(test$Hazardous, pred_glm_weighted_06)
mean(pred_glm_weighted_06!=test$Hazardous)
```

![](./confusion_glm_w.png){#id .class width=100% height=100%}

From the obtained results we can see that the method that we use for balancing the dataset doesn't change the results in terms of goodness of classification of our models. Then, we have the possibility to obtain at most the same results using less variables. For that, we choose to continue to consider only the over-sampling and under-sampling technique.

### Logistic Model with Balancing and Stepsize

To conclude with the Logistic Regression models we try to combine the two ideas: we develop a model using the *Stepwise* across the variables selected starting from the balanced dataset.

```{r, include = TRUE, warning = FALSE, messagge = FALSE}

# Application of the Stepwise method :

# (Here we don't re-apply the VIF method because we start from the
# previous result.)

#We start from the glm_bal model where we have already applied the
# VIF process, then we proceed with the Stepwise method.

glm_bal_step<- stepAIC(glm_bal, direction = "both", 
                        trace = FALSE)

# Observing the summary of the model:

summary(glm_bal_step)


# Computing the predictions with the model on the test set:

pred_glm_bal_step<- predict(glm_bal_step, test, type = "response")


# Converting the predictions in {0,1} according to the chosen threshold:

pred_glm_bal_step_04 = ifelse(pred_glm_bal_step > threshold4, 1, 0)
pred_glm_bal_step_05 = ifelse(pred_glm_bal_step > threshold5, 1, 0)
pred_glm_bal_step_06 = ifelse(pred_glm_bal_step > threshold6, 1, 0)
```

The significant variables now are:

* Absolute Magnitude
* Minimum Orbit Intersection
* Orbit Uncertainity
* Orbital Period

As we can see, the maintained covariates are the same of the model with only tha balancing of the classes. The Stepwise approach also in this case doesn't contribute to change the structure of the model.

```{r, include = TRUE, results = "hide", fig.show = "hide"}
# Confusion matrix with threshold = 0.4

table(test$Hazardous, pred_glm_bal_step_04)
mean(pred_glm_bal_step_04!=test$Hazardous)

# Confusion matrix with threshold = 0.5

table(test$Hazardous, pred_glm_bal_step_05)
mean(pred_glm_bal_step_05!=test$Hazardous)

# Confusion matrix with threshold = 0.6

table(test$Hazardous, pred_glm_bal_step_06)
mean(pred_glm_bal_step_06!=test$Hazardous)
```

![](./confusion_glm_bal_step.png){#id .class width=100% height=100%}

Comparing the results obtained with the balanced dataset without and with the stepwise, we can see that the difference is minimum: in particular they have the same variables, so the contribute of the step is considerable irrelevant.

We can compare the ability of classification of the models, looking at how they are able to reproduce the original graphical classification of the points that we can see in the following plots.

```{r,include = TRUE, fig.show = "hide"}


Haz_test<- test$Hazardous

# First we present the original classification :

ggplot(test, aes(x = Absolute_Magnitude,
                  y = Minimum_Orbit_Intersection,
                  color = Haz_test)) +
  geom_point()+
  labs(x = "Absolute Magnitude",
       y = "Minimum Orbit Intersection",
       color = "Hazardous")  +
  theme(legend.position = c(0.8, 0.8))

```

```{r,include = TRUE, fig.show = "hide"}
# The we try to reproduce the same plot as above, considering the
# classifications obtained with the models

a<- ggplot(test, aes(x = Absolute_Magnitude,
                  y = Minimum_Orbit_Intersection,
                  color = as.factor(pred_glm_compl_04))) +
  geom_point()+
  labs(x = "Absolute Magnitude",
       y = "Minimum Orbit Intersection",
       color = "Hazardous",
       title = "Simple GLM : 0.4")  +
  theme(legend.position = c(0.8, 0.8))



b<- ggplot(test, aes(x = Absolute_Magnitude,
                  y = Minimum_Orbit_Intersection,
                  color = as.factor(pred_glm_compl_step_04))) +
  geom_point()+
  labs(x = "Absolute Magnitude",
       y = "Minimum Orbit Intersection",
       color = "Hazardous",
       title = "GLM with Stepwise : 0.4")  +
  theme(legend.position = c(0.8, 0.8))



c<- ggplot(test, aes(x = Absolute_Magnitude,
                  y = Minimum_Orbit_Intersection,
                  color = as.factor(pred_glm_bal_06))) +
  geom_point()+
  labs(x = "Absolute Magnitude",
       y = "Minimum Orbit Intersection",
       color = "Hazardous",
       title = "GLM with Balanced dataset : 0.6")  +
  theme(legend.position = c(0.8, 0.8))


d<- ggplot(test, aes(x = Absolute_Magnitude,
                  y = Minimum_Orbit_Intersection,
                  color = as.factor(pred_glm_bal_step_06))) +
  geom_point()+
  labs(x = "Absolute Magnitude",
       y = "Minimum Orbit Intersection",
       color = "Hazardous",
       title = "GLM with Balanced dataset and Stepwise : 0.6")  +
  theme(legend.position = c(0.8, 0.8))


grid.arrange(a, b, c, d, nrow = 2)
```

![](./AM_MOI_test.png){#id .class width=80% height=80%}

![](./AM_MOI_glms.png)

As we said looking at the confusion matrices computed for the different models, we can see also graphically that the main difference is visible between models trained on the original dataset and the ones trained on the balanced set of data. In particular here we can also see that the *Stepwise* approach doesn't help to produce better classifications.
Focusing on the bottom figures, we can see that, comparing the results with the real differentiation of bodies, we classify as *Hazardous* more observations that in reality are *Not Hazardous*, but at the same moment we are able to classify almost all the dangerous units as they are. In any case, the difference between all the *GLM* models is not so relevant.


```{r,include = TRUE, fig.show = "hide"}
# We compare the results obtained with the four different models, plotting
# now an estimation of the logistic curve using the predictions given by
# the models:

predicted_data<- data.frame(prob.of.Haz = pred_glm_compl, Haz = test$Hazardous)
predicted_data<- predicted_data[order(predicted_data$prob.of.Haz, decreasing = FALSE),]
predicted_data$rank<-  1:nrow(predicted_data)

a<- ggplot(data = predicted_data, aes(x = rank, y = prob.of.Haz)) +
  geom_point(aes(color = as.factor(Haz)), alpha = 1, shape = 1, stroke = 1) +
  xlab("Index")+
  ylab("Predicted probability")+
  ggtitle("Estimated Logistic Curve - Simple GLM")


predicted_data<- data.frame(prob.of.Haz = pred_glm_compl_step, Haz = test$Hazardous)
predicted_data<- predicted_data[order(predicted_data$prob.of.Haz, decreasing = FALSE),]
predicted_data$rank<- 1:nrow(predicted_data)

b<- ggplot(data = predicted_data, aes(x = rank, y = prob.of.Haz)) +
  geom_point(aes(color = as.factor(Haz)), alpha = 1, shape = 1, stroke = 1) +
  xlab("Index")+
  ylab("Predicted probability")+
  ggtitle("Estimated Logistic Curve - GLM with Stepwise")



predicted_data<- data.frame(prob.of.Haz = pred_glm_bal, Haz = test$Hazardous)
predicted_data<- predicted_data[order(predicted_data$prob.of.Haz, decreasing = FALSE),]
predicted_data$rank<- 1:nrow(predicted_data)

c<- ggplot(data = predicted_data, aes(x = rank, y = prob.of.Haz)) +
  geom_point(aes(color = as.factor(Haz)), alpha = 1, shape = 1, stroke = 1) +
  xlab("Index")+
  ylab("Predicted probability")+
  ggtitle("Estimated Logistic Curve - GLM with Balanced data")



predicted_data<- data.frame(prob.of.Haz = pred_glm_bal_step, Haz = test$Hazardous)
predicted_data<- predicted_data[order(predicted_data$prob.of.Haz, decreasing = FALSE),]
predicted_data$rank<- 1:nrow(predicted_data)

d<- ggplot(data = predicted_data, aes(x = rank, y = prob.of.Haz)) +
  geom_point(aes(color = as.factor(Haz)), alpha = 1, shape = 1, stroke = 1) +
  xlab("Index")+
  ylab("Predicted probability")+
  ggtitle("Estimated Logistic Curve - GLM with Balanced data and Stepwise")


library(gridExtra)
grid.arrange(a, b, c, d, nrow = 2)

```

![](./log_est_glms.png)
In the plot we put in the *x-axis* our observations sorted considering their probability to be dangerous, in the *y-axis* we have instead the predicted probability. We observe that in all models, highest levels of probability bring all the model to classify the observations related as *Hazardous*. In the middle of the curves we find those values trivial to classify, from which we can see the different performances of the proposed models. Observing the colors proportions, we see that with the balanced dataset used for the training we are able to respect more the correct proportion of *Hazardous* bodies present in the test set. In fact, from the graph and from the previous confusion matrix, we can see that the balanced dataset is able to classify the celestial bodies labeled Hazardous much better also because it has many more observations of that class to train on.



## Discriminant Analysis
Besides the generalized linear models, other useful models for our classification problem could be *Linear Discriminant Analysis* and *Quadratic Discriminant Analysis* which are based on Bayes theorem and try different approach for classification.
Before being able to apply them, it is necessary to check some assumptions underlying these methodologies.

The most important, that we check before starting with the modelling, is the normality of the variables conditioned to the two classes *Hazardous* and *Not Hazardous*.

### Normality Requirement of the covariates
We introduce at this point the *Shapiro-Wilk test* that's the most powerful test for verifying the normality. 
It's used for the verification of statistic hypothesis, where in particular the null hypothesis is represented by the normality of the population distribution; if and only if the value of *p* is equal to or less than 0.05, then the hypothesis of normality will be rejected by the Shapiro test. On failing, the test can state that the data will not fit the distribution normally with 95% confidence.

```{r,include = TRUE, messagge = FALSE, fig.show = "hide"}

# We apply the Shapiro - Wilks test on each covariate, considering
# the two different classes:

shapiro.test(train_balanced$Absolute_Magnitude[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Absolute_Magnitude[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Est_Dia_in_KM_max[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Est_Dia_in_KM_max[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Relative_Velocity_km_per_hr[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Relative_Velocity_km_per_hr[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Miss_Dist_Astronomical[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Miss_Dist_Astronomical[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Orbit_Uncertainity[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Orbit_Uncertainity[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Minimum_Orbit_Intersection[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Minimum_Orbit_Intersection[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Jupiter_Tisserand_Invariant[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Jupiter_Tisserand_Invariant[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Eccentricity[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Eccentricity[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Semi_Major_Axis[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Semi_Major_Axis[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Inclination[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Inclination[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Asc_Node_Longitude[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Asc_Node_Longitude[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Orbital_Period[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Orbital_Period[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Perihelion_Distance[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Perihelion_Distance[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Perihelion_Arg[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Perihelion_Arg[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Mean_Anomaly[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Mean_Anomaly[train_balanced$Hazardous==1])
shapiro.test(train_balanced$Mean_Motion[train_balanced$Hazardous==0])
shapiro.test(train_balanced$Mean_Motion[train_balanced$Hazardous==1])

```

In order to try to obtain artificially the normality for some variables, we have also tried to apply some transformations to the same features. In particular, we tried the *Linear Scaling* method and the *Logarithmic* function, but the results remain the same: the hypothesis of normality is rejected by the tests.

Although this assumption is not satisfied, we try in any case to apply the models to our data, aware of the potential gaps that these could show.

## Linear Discriminant Analysis
With Linear Discriminant Analysis (LDA), we aim to find a linear combination of features that characterizes the two classes *Hazardous* and *Not Hazardous*, where the resulting combination will be used as a linear classifier.

In the application of the LDA method we have to take in account that it works when the measurements are made on independent variables. For that reason, the model that will be our starting point will not consider the variables with which we have the problem of collinearity: we re-use the information obtained in the previous models.
(The same holds for the Quadratic Discriminant Analysis that we will see in the next section).

We also know, that this type of analysis is quite sensitive to the presence of *outliers*, so in this case we proceed try to find them, looking at the variables summary and boxplots, and we will consider their removal. Looking at the distribution of the variables, we decide to remove the most extreme outlier values and to do that we use the following code:

```{r,include = TRUE, messagge = FALSE, fig.show = "hide"}
# We consider the previous output given by the summary of the dataset.


# We report here some examples of outliers removal. Then we will 
# define the new training dataset without these examples.

# Absolute Magnitude

g1<- ggplot(data = train, aes(y = Absolute_Magnitude,fill = 2)) +
       geom_boxplot(outlier.colour = "red", outlier.shape = 16,
                    outlier.size = 2)+
       theme(legend.position="none") +
       ylab("Absolute Magnitude")

# We look for the presence of outliers:

chisq.out.test(train$Absolute_Magnitude)

# Removal of the found outlier:

which(train$Absolute_Magnitude == 11.16) # 256


# We do the same:

g2<- ggplot(data = train, aes(y = Est_Dia_in_KM_max,fill = 2)) +
       geom_boxplot(outlier.colour = "red", outlier.shape = 16,
                    outlier.size = 2)+
       theme(legend.position="none") +
       ylab("Estimated Diameter")

chisq.out.test(train$Est_Dia_in_KM_max)

which(train$Est_Dia_in_KM_max == 34.836938254) # 256


g3<- ggplot(data = train, aes(y = Orbital_Period,fill = 2)) +
       geom_boxplot(outlier.colour = "red", outlier.shape = 16,
                    outlier.size = 2)+
       theme(legend.position="none") +
       ylab("Orbital Period")

chisq.out.test(train$Orbital_Period)

which(train$Orbital_Period == 2912.0220196159) # 1556

grid.arrange(g1, g2, g3, nrow = 1)

```

![](./boxplots.png)

```{r,include = FALSE, messagge = FALSE}
train<- train[-c(256,1556),]
```

Another note needs to be done regarding a possible "features selection" with the followings models. For LDA we can easily observe the values of the coefficients estimated for each variable, while for the QDA it's more difficult. For that reason we will consider all the set of variables.


### Simple LDA
We define now a *Simple LDA model*, as we said before, starting from the knowledge about the best subset of variables to use deriving from the analysis conduced with the use of *VIF*.

```{r, include = TRUE, messagge = FALSE}
library(MASS)

# Model definition:

lda_compl<- lda(Hazardous ~ . - Aphelion_Dist - Semi_Major_Axis - 
    Jupiter_Tisserand_Invariant - Eccentricity - Mean_Motion - 
    Est_Dia_in_KM_max, family = "binomial", data = train)

# Observing the model summary:

lda_compl

# Computing predictions:

pred_lda_compl<- predict(lda_compl, test, type = "response") # threshold: 0.5
post_lda_compl<- pred_lda_compl$posterior

# Converting the predictions in {0,1} according to the chosen threshold:

pred_lda_compl_04<- as.factor(ifelse(post_lda_compl[,2] > threshold4, 1, 0))
pred_lda_compl_05<- pred_lda_compl$class
pred_lda_compl_06<- as.factor(ifelse(post_lda_compl[,2] > threshold6, 1, 0))
```
From the call to the summary method what we see is that the variables that show greatest coefficients are:

* Absolute Magnitude
* Missing Distance
* Orbit Uncertainity
* Minimum Orbit Intersection

We see now how the combination of these variables can perform the classification.


```{r, include = TRUE, results = "hide", fig.show = "hide"}

# Confusion matrix with threshold = 0.4

table(test$Hazardous, pred_lda_compl_04)
mean(pred_lda_compl_04!=test$Hazardous)

# Confusion matrix with threshold = 0.5

table(test$Hazardous, pred_lda_compl_05)
mean(pred_lda_compl_05!=test$Hazardous)

# Confusion matrix with threshold = 0.6

table(test$Hazardous, pred_lda_compl_06)
mean(pred_lda_compl_06!=test$Hazardous)

```

With the same thresholds used until now, the results obtained are worst in terms of false negative. We can try less values and to observe if we are able to make better the model's performance:

```{r, include = TRUE, message = FALSE}

pred_lda_compl_02<- as.factor(ifelse(post_lda_compl[,2] > 0.2, 1, 0))
pred_lda_compl_03<- as.factor(ifelse(post_lda_compl[,2] > 0.3, 1, 0))

# Confusion matrix with threshold = 0.2

table(test$Hazardous, pred_lda_compl_02)
mean(pred_lda_compl_02!=test$Hazardous)

# Confusion matrix with threshold = 0.3

table(test$Hazardous, pred_lda_compl_03)
mean(pred_lda_compl_03!=test$Hazardous)

```

![](./confusion_lda.png){#id .class width=100% height=100%}

We can observe here that we have two good results: with thresholds 0.3 and 0.4. If we analyze better the matrices, we can see that for the first the general error is greater but we have a less rate of *False Negative*. In the other case, instead we have a slightly less general error but the rate of *False Negative* is higher. Considering our aim for the classification of dangerous bodies for the Earth, in this case we are most interested in a model like the one with the threshold 0.3.

```{r, include = TRUE, message = FALSE, fig.show = "hide"}

# We use now the information given by:
# - x: linear combination of the variables that better describe the examples
# - class: assigned class

ldahist(pred_lda_compl$x[,1], g = pred_lda_compl$class, col = 2)
```

![Histograms of how the combinations of variables classify the examples](./hist_lda.png){#id .class width=80% height=80%}

The results of the linear discriminant analysis are well explained in the above graph. This represents the values of the discriminant function previously calculated for the observations of the two different groups (Hazardous and Not Hazardous). We can see that the function discriminates the two groups quite well, having only an overlap when it assumes greater values, which however belong more to the class of dangerous celestial bodies.


### LDA with balanced data

We apply now the same process on the balanced dataset.

```{r, include = TRUE, fig.show = "hide"}
# We consider the previous output given by the summary of the dataset.

# We report here some examples of outliers removal. Then we will 
# define the new training dataset without these examples.

# Absolute Magnitude

g4<-ggplot(data = train_balanced, aes(y =     Relative_Velocity_km_per_hr,fill = 2)) +
       geom_boxplot(outlier.colour = "red", outlier.shape = 16,
                    outlier.size = 2)+
       theme(legend.position="none") +
       ylab("Relative Velocity")

# We look for the presence of outliers:

chisq.out.test(train_balanced$Relative_Velocity_km_per_hr)

# Removal of the found outlier:

which(train_balanced$Relative_Velocity_km_per_hr >= 160681.487851189) # 2313 2580


# We do the same:

g5<- ggplot(data = train_balanced, aes(y = Est_Dia_in_KM_max,fill = 2)) +
       geom_boxplot(outlier.colour = "red", outlier.shape = 16,
                    outlier.size = 2)+
       theme(legend.position="none") +
       ylab("Estimated Diameter")

chisq.out.test(train_balanced$Est_Dia_in_KM_max)

which(train_balanced$Est_Dia_in_KM_max == 34.836938254) # 1301 1703


g6<- ggplot(data = train_balanced, aes(y = Inclination,fill = 2)) +
       geom_boxplot(outlier.colour = "red", outlier.shape = 16,
                    outlier.size = 2)+
       theme(legend.position="none") +
       ylab("Inclination")

chisq.out.test(train_balanced$Inclination)

which(train_balanced$Inclination >= 75.406666841) # 3044

grid.arrange(g4, g5, g6, nrow = 1)

```

![](./boxplots2.png)

```{r,include = FALSE, messagge = FALSE}
train_balanced<- train_balanced[-c(1301, 1703, 2313, 2580, 3044),]
```

```{r, include = TRUE}

# Model definition starting from the previous glm_bal model:

lda_bal<- lda(data = train_balanced,
              Hazardous ~.-Aphelion_Dist-Semi_Major_Axis-
              Jupiter_Tisserand_Invariant-Eccentricity-
              Est_Dia_in_KM_max-Mean_Motion,
              family = "binomial")

# Observing the model summary:

lda_bal


# Computing the predictions with the model on the test set:

pred_lda_bal<- predict(lda_bal, test, type = "response")
post_lda_bal<- pred_lda_bal$posterior


# Converting the predictions in {0,1} according to the chosen threshold:

pred_lda_bal_03<- as.factor(ifelse(post_lda_bal[,2] > 0.3, 1, 0))
pred_lda_bal_04<- as.factor(ifelse(post_lda_bal[,2] > threshold4, 1, 0))
pred_lda_bal_05<- pred_lda_bal$class
pred_lda_bal_06<- as.factor(ifelse(post_lda_bal[,2] > threshold6, 1, 0))

```
With the use of the balanced dataset, the variables that most influence the discrimination are:

* Absolute Magnitude
* Missing Distance
* Orbit Uncertainity
* Minimum Orbit Intersection

and they are the same as before.

Let's see if the confusion matrices can give us more details about the classification performed by this model.


```{r, include = TRUE, results = "hide", fig.show = "hide"}

# Confusion matrix with threshold: 0.3

table(test$Hazardous, pred_lda_bal_03)
mean(pred_lda_bal_03!=test$Hazardous)

# Confusion matrix with threshold: 0.4

table(test$Hazardous, pred_lda_bal_04)
mean(pred_lda_bal_04!=test$Hazardous)

# Confusion matrix with threshold: 0.5

table(test$Hazardous, pred_lda_bal_05)
mean(pred_lda_bal_05!=test$Hazardous)

# Confusion matrix with threshold: 0.6

table(test$Hazardous, pred_lda_bal_06)
mean(pred_lda_bal_06!=test$Hazardous)

ldahist(pred_lda_bal$x[,1], g = pred_lda_bal$class, col = 2)
```

![](./confusion_lda_bal.png){#id .class width=100% height=100%}

Althought we computed the predictions also for the threshold 0.3, here we have better results with the other values. With the considered thresholds we have in any case the *False Negative* rate less than the *False Positive* one. The best result here is obtained with the threshold equal to 0.6: the total error is the less one, but we have an increased error on the classification of the hazardous element as *Not Hazardous*. If we compare the differences between the two errors, we may agree that the difference between the two false negative rate is less than the difference in terms of general error: in this case we prefer the last classification.

![Histograms of how the combinations of variables classify the examples](./hist_lda_bal.png){#id .class width=80% height=80%}

Even in the case of balanced data, through the following graph we can see that the function discriminates quite well the two classes, associating the class of dangerous celestial bodies to values greater than 0.

## Quadratic Discriminant Analysis
We implement now also a model using the *QDA* method with which we should be able to observe better results in general than *LDA*: LDA, in fact, tends to works well where there are few training observations and for that we need to estimate fewer parameters. In this case it's also important to reduce the variance.
In the other hand, if the set of observations is larger, the amount of variance is not relevant: we can use QDA in order to have the possibility to estimate more parameters and to have a more flexible model.

### Simple QDA

```{r, include = TRUE, messagge = FALSE}

# Model definition starting from the previous glm_compl:

qda_compl<- qda(Hazardous ~ . - Aphelion_Dist - Semi_Major_Axis - 
    Jupiter_Tisserand_Invariant - Eccentricity - Mean_Motion - 
    Est_Dia_in_KM_max- Orbit_Uncertainity, 
    family = "binomial", data = train)


# Computing predictions:

pred_qda_compl<- predict(qda_compl, test, type = "response") # threshold: 0.5
post_qda_compl<- pred_qda_compl$posterior


# Converting the predictions in {0,1} according to the chosen threshold:

pred_qda_compl_03<- as.factor(ifelse(post_qda_compl[,2] > 0.3, 1, 0))
pred_qda_compl_04<- as.factor(ifelse(post_qda_compl[,2] > threshold4, 1, 0))
pred_qda_compl_05<- pred_qda_compl$class
pred_qda_compl_06<- as.factor(ifelse(post_qda_compl[,2] > threshold6, 1, 0))
```

```{r, include = TRUE, results = "hide", fig.show = "hide"}

# Confusion matrix with threshold: 0.3

table(test$Hazardous, pred_qda_compl_03)
mean(pred_qda_compl_03!=test$Hazardous)

# Confusion matrix with threshold: 0.4

table(test$Hazardous, pred_qda_compl_04)
mean(pred_qda_compl_04!=test$Hazardous)

# Confusion matrix with threshold: 0.5

table(test$Hazardous, pred_qda_compl_05)
mean(pred_qda_compl_05!=test$Hazardous)

# Confusion matrix with threshold: 0.6

table(test$Hazardous, pred_qda_compl_06)
mean(pred_qda_compl_06!=test$Hazardous)

```

From this confusion matrices, we note that considering high threshold as for example 0.6, the rate of *False Positive* becomes less than the *False Negatives*. In order to compare good results for the *Simple QDA* model we consider less thresholds, starting also here from 0.3 up to 0.5 .

![](./confusion_qda.png){#id .class width=100% height=100%}

The values that we can observe with QDA are considerable: we have a very low value of general error and at the same time the rate of *False Negative* is the less error present from the model. In particular, if we have to choose the best option for the threshold, here we will choose 0.4


### QDA with balanced data

```{r, include = TRUE}
# Model definition starting from the previous glm_bal model:

qda_bal<- qda(data = train_balanced,
              Hazardous ~.-Aphelion_Dist-Semi_Major_Axis-
              Jupiter_Tisserand_Invariant-Eccentricity-
              Est_Dia_in_KM_max-Mean_Motion- Orbit_Uncertainity,
              family = "binomial")

# Observing the model summary:

qda_bal


# Computing the predictions with the model on the test set:

pred_qda_bal<- predict(qda_bal, test, type = "response")
post_qda_bal<- pred_qda_bal$posterior


# Converting the predictions in {0,1} according to the chosen threshold:

pred_qda_bal_03<- as.factor(ifelse(post_qda_bal[,2] > 0.3, 1, 0))
pred_qda_bal_04<- as.factor(ifelse(post_qda_bal[,2] > threshold4, 1, 0))
pred_qda_bal_05<- pred_qda_bal$class
pred_qda_bal_06<- as.factor(ifelse(post_qda_bal[,2] > threshold6, 1, 0))

```

```{r, include = TRUE, results = "hide", fig.show = "hide"}

# Confusion matrix with threshold: 0.3

table(test$Hazardous, pred_qda_bal_03)
mean(pred_qda_bal_03!=test$Hazardous)

# Confusion matrix with threshold: 0.4

table(test$Hazardous, pred_qda_bal_04)
mean(pred_qda_bal_04!=test$Hazardous)

# Confusion matrix with threshold: 0.5

table(test$Hazardous, pred_qda_bal_05)
mean(pred_qda_bal_05!=test$Hazardous)

# Confusion matrix with threshold: 0.6

table(test$Hazardous, pred_qda_bal_06)
mean(pred_qda_bal_06!=test$Hazardous)

```

![](./confusion_qda_bal.png)

Also in this case we have good results, but we can observe that the total error is generally increased. In the other hand, the *False Negative* rate is almost inexistent, and this brings the QDA model to be considered a good solution for our classification task.


## Regularized Regression
In this section we fit generalized models where we add some *penalizations*, introducing *Lasso* and *Ridge* regularizations.
They are two different statistical methods that allow us to compute an automatic selection of variables, operating shrinkage on the coefficients of the predictors in such a way that they assume values very close to zero or even zero.
When we have estimators that tend to have very large variants and small bias, we can also have presence of multicollinearity. However, estimators that have very large variants will produce poor estimates. This phenomenon is referred to as *Overfitting*. 

According to the results obtained until now, we decide to continue to test our model only on the balanced dataset.

Regarding the features selection step, we need to do some considerations.
For the *Lasso* model we have not to implement it because, thanks to its formulation, it does variable selection in the process of shrinking to zero the values of coefficients of variables.
For the *Ridge* we have basically the same behaviour with the only difference that here the coefficients are not directly set to zero, as in the lasso, but shrinked more and more until reaching zero.

### Ridge Regression
```{r, include = TRUE, results = "hide"}

# We use here the balanced training dataset because we have seen that, generally, we
# obtain better results.

train_bal_mat<- as.matrix(train_balanced[,-19])
test_mat<- as.matrix(test[,-19])
  
# We look for the best value for lambda in order to define the best model:

# We use cross validation glmnet

ridge_cv <- cv.glmnet(train_bal_mat, train_balanced$Hazardous,
                      alpha = 0, family = "binomial", type.measure = "class")

plot(ridge_cv)

# We identify th best lambda value

lambda_opt_ridge <- ridge_cv$lambda.min
lambda_opt_ridge

# We compute predictions with the ridge model using the best value for 
# lambda that we have obtained

pred_ridge<- predict(ridge_cv, test_mat, type = "class", s = lambda_opt_ridge)

table(test$Hazardous, pred_ridge)

```

### Lasso Regression

```{r, include = TRUE, results = "hide"}

# We look for the best value for lambda in order to define the best model:

# We use cross validation glmnet

lasso_cv <- cv.glmnet(train_bal_mat, train_balanced$Hazardous, 
                      alpha = 1, family = "binomial", type.measure = "class")

plot(lasso_cv)

# We identify th best lambda value

lambda_opt_lasso <- lasso_cv$lambda.min
lambda_opt_lasso

# We compute predictions with the lasso model using the best value for 
# lambda that we have obtained

pred_lasso<- predict(lasso_cv, test_mat, type = "class", s = lambda_opt_lasso)

table(test$Hazardous, pred_lasso)

```

![](./confusion_ridge_lasso.png){#id .class width=80% height=80%}

We can now compare the performances of these two regularization methods.
As the matrices explain, using the *Lasso* alternative, we are able to reach both a total error and a *False Negative* rate less than using the *Ridge* model.


## K-Nearest Neighbors 
The *K-Nearest Neighbor* (KNN) algorithm is a completely non-parametric approach that represents one of the most widely used algorithms. In particular, here, no assumptions are made about the shape of the decision boundary.
To make a prediction for an observation *x* using *KNN* we need to:

* identify the *k* training observations that are closest to *x*;
* assigning *x* to the class to which the plurality of these observations belong.

```{r, include = TRUE, fig.show = "hide"}

# Function for linear scaling our features

min_max_norm <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

# We normalize the columns

nasa_n <- as.data.frame(lapply(nasa[,-19], min_max_norm))

# We re-add the Hazardous features to re-complete the dataset

nasa_n$Hazardous <- nasa$Hazardous

# We re-do the split obtaining the same differentiation as before

set.seed(0607)

split_n <- initial_split(nasa_n, prop = 0.75) 
train_n <- training(split_n)
test_n <- testing(split_n)

# We balance again our dataset

train_n_balanced<- ovun.sample(Hazardous~., data = train_n, 
                             method = "under", p = 1/5.35,
                              seed = 1)$data

# We implement a first KNN model removing the variables that show collinearity

library(class)

# We look now for the best value of the parameter k

kmax <- 100

test_error <- numeric(kmax)

# For each possible value of k we consider the obtained accuracy of the model

for (k in 1:kmax) {
  knn_pred <- as.factor(knn(train_n_balanced[,-c(2,7,8,9,15,18,19)],
                            test_n[,-c(2,7,8,9,15,18,19)], 
                            cl = train_n_balanced$Hazardous, k = k))
  
  cm <- confusionMatrix(data = knn_pred, reference = as.factor(test_n$Hazardous))
  
  test_error[k] <- 1 - cm$overall[1]
}

# We took the minimum value of the error

k_min <- which.min(test_error)

# We compute now the prediction with the value of k that gives us the minimum error

knn<- knn(train_n_balanced[,-c(2,7,8,9,15,18,19)], test_n[,-c(2,7,8,9,15,18,19)], 
          cl = train_n_balanced$Hazardous, k = k_min)

knn_pred_min <- as.factor(knn)

# Confusion matrix for KNN on the test set

tab<- table(test_n$Hazardous, knn)
tab
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)


cm <- confusionMatrix(data = knn_pred_min, reference = as.factor(test_n$Hazardous))

ggplot(data.frame(test_error), aes(x = 1:kmax, y = test_error)) +
  geom_line(colour="blue") +
  geom_point(colour="blue") +
  xlab("K (#neighbors)") + ylab("Test error") +
  ggtitle(paste0("Best value of K = ", k_min,
                 " (minimal error = ",
                 format((test_error[k_min])*100, digits = 4), "%)"))



ggplot(test, aes(x = Absolute_Magnitude,
                     y = Minimum_Orbit_Intersection,
                     color = as.factor(knn))) +
  geom_point()+
  labs(x = "Absolute Magnitude",
       y = "Minimum Orbit Intersection",
       color = "Hazardous",
       title = "KNN with K = 9")  +
  theme(legend.position = c(0.8, 0.8))


```
![](./blueplot.png)

To achieve the best results we can get from knn, we will look for the optimal value of K by calculating the error rate value on the test data on multiple values of K and choosing the value that minimizes that error.

K = 9 seems to provide the lowest test error rate (around 11%) so we choose this as the value for K to evaluate our classification.
We can represent the resulting classification in the image below.

![](./knn9.png)

How we can see, the classification done with this method is far from the ones obtained with the other models. In particular, comparing the plot with the true values of *Hazardous* and *Not Hazardous*, we observe that we have points classified as dangerous with not a "clear decision boundary" defined by the same model, while in the original classification we have like a "unique block" of dangerous bodies.
This brings the model to have such a bad result.

## Model Considerations

We can now summarize the obtained results with all the types of models
we have implemented, choosing particular metrics in order to compare
them and their performances on the same level. Referring to the nature
of the presented problem of classification of hazardous NEOs for the
Earth, we decide to consider the *Overall Error* rate and at the same
time the *False Negative* rate. Doing in this way we are to understand
the general correctness of the predictions of the models and their
safety: as we said in the initial presentation, we prefer, in fact, to
have an higher proportion of *Not Hazardous* bodies classified as
*Hazardous* instead of higher quantities of *Hazardous* classified as
*Not Hazardous*.
As we can see in particular, the trial with a non-parametric model doesn't bring such a good result, so in the final comparison we will see only the previous models.

### Comparison of Models

![](./model_results_table.png)

From the summarizing table we can do some considerations:

* using the balancing of the dataset we are able to improve the results of the model in terms of *False Negative*, while we can observe an increasing of the *Overall Error* rate;
* the use of the *Stepwise* approach doesn't bring improvements of goodness of the classification;

* for the GLM method, the best rates are given by the Logistic Regression applied on the balanced dataset and using the threshold equal to 0.4: considering a slightly less threshold against the classic 0.5, we optimize for recall;

* for LDA we choose the model trained on the balanced data for which we classify the observation using 0.5 as threshold: in this case, in fact, we consider the option with less value of the general error, beacuse the difference in terms of false negative rate is not so relevant;

* for QDA we chose the model that is trained on the original dataset and uses as threshold 0.3. Also the model trained on the balanced dataset and 0.6 as value could be a good option: also in this case, in order to choose the best one between them, we consider the difference in terms of both overall error rate and false negative rate. The greatest distance between the proposed model is about the general error, so we decide to maintain QDA trained on the unbalanced dataset;

* between the two regularization methods, we can observe that Lasso is the best performing one, with half of overall error rate compared with the error computed by Ridge regression;

* at the end, we consider the results obtained with KNN method. Our aim is to minimize both the overall error rate and the false negative rate: in this case we have an error of classification of the hazardous NEOs that's around 0.40. This issue is produced by the use of a low value for the hyperparamer k with which we have the highest accuracy score.


### ROC Curve
We now introduce the ROC curve tool, a curve capable of showing the performance of a model, which is obtained plotting the *False Positive Rate*, on the x-axis, and *True Positive Rate*, on y-axis, on the Cartesian reference system. In order to understand which is the best model, we consider the area under the curve: greater is the area, better is the model's performance.
  
```{r, include = TRUE, results = "hide", messagge = FALSE, warning = FALSE}

# Best GLM model

glm_best<- glm(data = train_balanced,
              Hazardous ~ Absolute_Magnitude+Minimum_Orbit_Intersection+
                Orbit_Uncertainity+Orbital_Period,
              family = "binomial")

pred_glm_best<- predict(glm_best, test, type = "response")

# Best LDA model

lda_best<- lda(data = train_balanced,
              Hazardous ~Absolute_Magnitude+Miss_Dist_Astronomical+
                Orbit_Uncertainity+Minimum_Orbit_Intersection,
              family = "binomial")

pred_lda_best<- predict(lda_best, test, type = "response")
post_lda_best<- pred_lda_best$posterior

# Best QDA model

qda_best<- qda(Hazardous ~ . - Aphelion_Dist - Semi_Major_Axis - 
    Jupiter_Tisserand_Invariant - Eccentricity - Mean_Motion - 
    Est_Dia_in_KM_max- Orbit_Uncertainity, 
    family = "binomial", data = train)

pred_qda_best<- predict(qda_best, test, type = "response")
post_qda_best<- pred_qda_best$posterior

# Best Ridge model

ridge_best<- glmnet(train_bal_mat, train_balanced$Hazardous, 
                    alpha = 0, family = "binomial", lambda = lambda_opt_ridge)

pred_ridge_best<- predict(ridge_best, test_mat, type = "response", s = lambda_opt_ridge)

# Best Lasso model

lasso_best<- glmnet(train_bal_mat, train_balanced$Hazardous, 
                    alpha = 0, family = "binomial", lambda = lambda_opt_lasso)

pred_lasso_best<- predict(lasso_best, test_mat, type = "response", s = lambda_opt_lasso)

```

```{r, include = TRUE, results = "hide"}
# We compare the best models of each type looking at the ROC curve

prediction <- tibble(truth = as.factor(test$Hazardous))
prediction <- prediction %>% mutate(pred = as.numeric(pred_glm_best))%>%
  mutate(model= "GLM")%>% 
  add_row(truth = as.factor(test$Hazardous), pred = as.numeric(post_qda_best[,2]), 
          model= "LDA")%>%
  add_row(truth = as.factor(test$Hazardous), pred = as.numeric(post_qda_best[,2]), 
          model= "QDA")%>%
  add_row(truth = as.factor(test$Hazardous), pred = as.numeric(pred_ridge_best), 
          model= "Ridge")%>%
  add_row(truth = as.factor(test$Hazardous), pred = as.numeric(pred_lasso_best), 
          model= "Lasso")


roc <- prediction %>% group_by(model) %>% 
  roc_curve(truth, pred, event_level = "second") %>% 
  autoplot()
roc
```

```{r, include = TRUE, message = FALSE, warning = FALSE}

auc(test$Hazardous, pred_glm_best)
auc(test$Hazardous, post_lda_best[,2])
auc(test$Hazardous, post_qda_best[,2])
auc(test$Hazardous, pred_ridge_best)
auc(test$Hazardous, pred_lasso_best)

```

Looking both the ROC-curve plot and the Area Under the Curve (AUC) we can deduce that the best model for our classification task is the QDA. If we have to consider the other models, we are in the position to say that they are all good models in terms of obtained accuracy, but in this case we have to make reference also at the above presented table. Here in fact we can see also the *False Negative* rate computed for each model and we can observe that for QDA, Lasso and Ridge we have the best result over all the other models. For that reason the final choice, driven by the considerations done across the full project are these.

```{r, include = TRUE, message = FALSE, warning = FALSE, fig.show = "hide"}

e<- ggplot(test, aes(x = Absolute_Magnitude, y =
Minimum_Orbit_Intersection, color = as.factor(ifelse(pred_glm_best > 0.5, 1, 0)))) +
geom_point()+ labs(x = "Absolute Magnitude", y = "Minimum Orbit
Intersection", color = "Hazardous", title = "GLM") + theme(legend.position = c(0.8, 0.8))

f<- ggplot(test, aes(x = Absolute_Magnitude, y =
Minimum_Orbit_Intersection, color =
as.factor(pred_qda_best$class))) + geom_point()+ labs(x = "Absolute
Magnitude", y = "Minimum Orbit Intersection", color = "Hazardous", title
= "LDA") + theme(legend.position =
c(0.8, 0.8))

g<- ggplot(test, aes(x = Absolute_Magnitude, y =
Minimum_Orbit_Intersection, color = as.factor(pred_qda_best$class))) +
geom_point()+ labs(x = "Absolute Magnitude", y = "Minimum Orbit
Intersection", color = "Hazardous", title = "QDA") + theme(legend.position = c(0.8, 0.8))

h<- ggplot(test, aes(x = Absolute_Magnitude, y =
Minimum_Orbit_Intersection, color = as.factor(ifelse(pred_ridge_best > 0.5, 1, 0)))) +
geom_point()+ labs(x = "Absolute Magnitude", y = "Minimum Orbit
Intersection", color = "Hazardous", title = "Ridge") + theme(legend.position = c(0.8,
0.8))

i<- ggplot(test, aes(x = Absolute_Magnitude, y =
Minimum_Orbit_Intersection, color = as.factor(ifelse(pred_lasso_best > 0.5, 1, 0)))) +
geom_point()+ labs(x = "Absolute Magnitude", y = "Minimum Orbit
Intersection", color = "Hazardous", title = "Lasso") + theme(legend.position = c(0.8,
0.8))

grid.arrange(e,f,g, h, i, nrow = 2)
```

![](./final_consideration.png)